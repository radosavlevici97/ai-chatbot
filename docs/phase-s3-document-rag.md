# Phase S3 — Document Intelligence / RAG

> **Timeline:** Week 3 | **Goal:** Upload documents, ask questions, get answers with citations
> **Depends on:** Phase S2 (conversations, message persistence)
> **Delivers to:** Phase S4

---

## 1. Objectives

| # | Objective | Acceptance Criteria |
|---|-----------|-------------------|
| 1 | File upload API | Upload PDF, DOCX, TXT; validate type/size; store via storage abstraction |
| 2 | Text extraction | Extract plain text from PDF (`pdf-parse`), DOCX (`mammoth`), TXT |
| 3 | Chunking | Split extracted text into overlapping chunks (paragraph-aware) |
| 4 | Embedding | Embed chunks via separated `EmbeddingProvider`, store in ChromaDB |
| 5 | RAG retrieval | Given a question, find top-k relevant chunks |
| 6 | Citation injection | Include source filename + page in LLM context and SSE response |
| 7 | Document library UI | Upload panel, file list (TanStack Query), processing status, delete |
| 8 | Per-conversation RAG toggle | Enable/disable "search my documents" |
| 9 | Citations in chat | Display source references below assistant messages |

---

## 2. Architecture — RAG Pipeline

```
User uploads file
        |
        v
+-------------------+
|  File Validation   |  <-- Allowed types, max size, magic bytes
|  & Storage         |  --> StorageAdapter.save() (local or R2)
+--------+----------+
         v
+-------------------+
|  Text Extraction   |  <-- pdf-parse (PDF), mammoth (DOCX), raw (TXT)
|                    |  --> Plain text + approximate page numbers
+--------+----------+
         v
+-------------------+
|  Chunking          |  <-- Recursive text splitter
|                    |  --> 1000 chars, 200 overlap, preserve paragraphs
+--------+----------+
         v
+-------------------+
|  Embedding         |  <-- getEmbeddingProvider() (always Gemini)
|                    |  --> 768-dim vectors, batched with concurrency control
+--------+----------+
         v
+-------------------+
|  Vector Store      |  <-- ChromaDB (embedded, in-process)
|                    |  --> Stored with metadata: userId, docId, page, section
+-------------------+

User asks question about documents
        |
        v
+-------------------+
|  Query Embedding   |  <-- Same EmbeddingProvider (Gemini text-embedding-004)
+--------+----------+
         v
+-------------------+
|  Vector Search     |  <-- ChromaDB similarity search, top-k=5
|                    |  --> Filter by userId (isolation)
+--------+----------+
         v
+-------------------+
|  Context Assembly  |  <-- Inject retrieved chunks into LLM prompt
|                    |  --> "Based on the following documents: ..."
+--------+----------+
         v
+-------------------+
|  LLM Generation    |  <-- getChatProvider() (Gemini or fallback)
|  + Citations       |  --> Answer with [Source: file.pdf, p.3]
+-------------------+
```

**Design Rationale:**
- Embedding uses `getEmbeddingProvider()` (always Gemini) — **never** `getChatProvider()`. Embeddings must remain consistent even when the chat provider falls back to OpenRouter, because vectors generated by different embedding models are incompatible.
- File storage goes through the `StorageAdapter` interface (S1) — swappable between local disk and Cloudflare R2 via `STORAGE_TYPE` env var.
- Each pipeline stage is a separate service — testable in isolation, replaceable independently.

---

## 3. Technology Choices for Node.js RAG

| Component | Library | Why |
|-----------|---------|-----|
| PDF extraction | **`pdf-parse`** | Pure JS, no native deps, works in Node |
| DOCX extraction | **`mammoth`** | Extracts clean text from .docx |
| Text chunking | **Custom splitter** | Simple recursive chunker — no heavy framework needed |
| Embedding | **`getEmbeddingProvider()`** | Separated from chat provider (S1); always Gemini |
| Vector DB | **`chromadb`** npm package | Embedded mode, in-process, persists to disk |
| File validation | **`file-type`** | Magic byte detection (not just extension) |
| File storage | **`StorageAdapter`** (S1) | Abstraction over local/R2 via `STORAGE_TYPE` |

### New Dependencies — `backend/package.json`

```json
{
  "pdf-parse": "^1.1.1",
  "mammoth": "^1.8.0",
  "chromadb": "^1.10.0",
  "file-type": "^19.6.0"
}
```

> **Note:** No `multer` needed — Hono's built-in `c.req.formData()` handles multipart file uploads natively.

---

## 4. Shared Document Schemas — `packages/shared/src/schemas/document.ts`

All document-related Zod schemas live in `@chatbot/shared` so both frontend and backend validate against the same types.

```typescript
import { z } from "zod";

// ── Upload ──────────────────────────────────────────
export const uploadResponseSchema = z.object({
  id: z.string(),
  originalFilename: z.string(),
  fileType: z.enum(["pdf", "docx", "txt"]),
  fileSize: z.number(),
  status: z.enum(["processing", "indexed", "failed"]),
  chunkCount: z.number(),
  createdAt: z.string(),
});

export type UploadResponse = z.infer<typeof uploadResponseSchema>;

// ── Document status ─────────────────────────────────
export const documentStatusSchema = z.object({
  status: z.enum(["processing", "indexed", "failed"]),
  chunkCount: z.number(),
  errorMessage: z.string().nullable(),
});

export type DocumentStatus = z.infer<typeof documentStatusSchema>;

// ── Document list item ──────────────────────────────
export const documentListItemSchema = z.object({
  id: z.string(),
  originalFilename: z.string(),
  fileType: z.enum(["pdf", "docx", "txt"]),
  fileSize: z.number(),
  status: z.enum(["processing", "indexed", "failed"]),
  chunkCount: z.number(),
  createdAt: z.string(),
});

export type DocumentListItem = z.infer<typeof documentListItemSchema>;

// ── Search ──────────────────────────────────────────
export const documentSearchSchema = z.object({
  query: z.string().min(1, "Search query is required").max(2000),
  topK: z.coerce.number().int().min(1).max(20).default(5),
});

export type DocumentSearchInput = z.infer<typeof documentSearchSchema>;

export const documentSearchResultSchema = z.object({
  text: z.string(),
  filename: z.string(),
  pageNumber: z.number(),
  relevance: z.number(),
});

export type DocumentSearchResult = z.infer<typeof documentSearchResultSchema>;
```

Also update the `sendMessageInputSchema` in `packages/shared/src/schemas/chat.ts`:

```typescript
// Add useDocuments flag to existing schema
export const sendMessageInputSchema = z.object({
  content: z.string().min(1).max(32_000),
  model: z.string().optional(),
  temperature: z.number().min(0).max(2).default(0.7),
  maxTokens: z.number().int().min(1).max(32_000).default(4096),
  useDocuments: z.boolean().default(false),  // NEW: enable RAG for this message
});
```

And re-export from `packages/shared/src/index.ts`:

```typescript
// Add to barrel export
export * from "./schemas/document.js";
```

**Design Rationale:**
- Schemas in `@chatbot/shared` ensure the frontend and backend agree on every field name, type, and constraint.
- `documentSearchSchema` validates the search endpoint input — prevents raw JSON body parsing.
- `topK` uses `z.coerce.number()` so query-string values (strings) auto-parse correctly.
- Export types alongside schemas — consumers get runtime validation AND compile-time types from one import.

---

## 5. Database Schema Additions

```typescript
// Add to backend/src/db/schema.ts

export const documents = sqliteTable("documents", {
  id: text("id").primaryKey(),
  userId: text("user_id").notNull().references(() => users.id, { onDelete: "cascade" }),
  filename: text("filename").notNull(),           // Stored filename (nanoid.ext)
  originalFilename: text("original_filename").notNull(), // User's original name
  fileType: text("file_type").notNull(),           // "pdf" | "docx" | "txt"
  fileSize: integer("file_size").notNull(),         // Bytes
  storagePath: text("storage_path").notNull(),      // Key in StorageAdapter
  status: text("status", {
    enum: ["processing", "indexed", "failed"],
  }).notNull().default("processing"),
  chunkCount: integer("chunk_count").default(0),
  errorMessage: text("error_message"),
  createdAt: text("created_at").notNull().default(sql`(datetime('now'))`),
}, (table) => [
  index("documents_user_created_idx").on(table.userId, table.createdAt),
]);

export const documentChunks = sqliteTable("document_chunks", {
  id: text("id").primaryKey(),
  documentId: text("document_id").notNull().references(() => documents.id, { onDelete: "cascade" }),
  chunkIndex: integer("chunk_index").notNull(),
  content: text("content").notNull(),
  pageNumber: integer("page_number"),
  tokenCount: integer("token_count"),
  chromaId: text("chroma_id").notNull(),  // ID in ChromaDB for deletion sync
  createdAt: text("created_at").notNull().default(sql`(datetime('now'))`),
}, (table) => [
  index("document_chunks_doc_idx").on(table.documentId),
]);
```

**Design Rationale:**
- `documents_user_created_idx` — composite index for the paginated document list query (`WHERE userId = ? ORDER BY createdAt DESC`).
- `storagePath` is a logical key, not a filesystem path — works with both `LocalStorageAdapter` and `R2StorageAdapter`.
- `chromaId` column enables sync between SQLite and ChromaDB on document deletion.

---

## 6. Backend Services

### 6.1 Storage Adapter — `src/lib/storage.ts` (defined in S1)

S1 defines the `StorageAdapter` interface and factory. S3 uses it for all file I/O. The key contracts:

```typescript
// Defined in S1 — referenced here for clarity
export interface StorageAdapter {
  save(path: string, buffer: Buffer): Promise<void>;
  read(path: string): Promise<Buffer>;
  delete(path: string): Promise<void>;
  exists(path: string): Promise<boolean>;
}

// Factory — returns adapter based on STORAGE_TYPE env var
export function getStorage(): StorageAdapter;
```

- `LocalStorageAdapter` — reads/writes to `UPLOAD_DIR` on the local filesystem.
- `R2StorageAdapter` — reads/writes to Cloudflare R2 bucket using S3-compatible API.
- `getStorage()` returns the correct one based on `env.STORAGE_TYPE`.

All file operations in S3 go through `getStorage()` — **never** `writeFileSync` / `readFileSync` directly.

### 6.2 File Upload & Validation — `src/services/upload.service.ts`

```typescript
import { fileTypeFromBuffer } from "file-type";
import { nanoid } from "nanoid";
import { env } from "../env.js";
import { getStorage } from "../lib/storage.js";
import { ValidationError } from "../lib/errors.js";
import { log } from "../middleware/logger.js";

const ALLOWED_TYPES: Record<string, string[]> = {
  "application/pdf": ["pdf"],
  "application/vnd.openxmlformats-officedocument.wordprocessingml.document": ["docx"],
  "text/plain": ["txt"],
};

const MAX_SIZE = () => env.UPLOAD_MAX_SIZE_MB * 1024 * 1024;

export async function validateAndStore(
  buffer: Buffer,
  originalFilename: string,
  userId: string,
  requestId: string,
): Promise<{ filename: string; fileType: string; storagePath: string; fileSize: number }> {
  // 1. Size check
  if (buffer.length > MAX_SIZE()) {
    throw new ValidationError(`File exceeds ${env.UPLOAD_MAX_SIZE_MB}MB limit`);
  }

  // 2. Magic byte detection
  const detected = await fileTypeFromBuffer(buffer);
  const ext = originalFilename.split(".").pop()?.toLowerCase() ?? "";

  let fileType: string;
  if (detected && ALLOWED_TYPES[detected.mime]) {
    fileType = ALLOWED_TYPES[detected.mime][0];
  } else if (ext === "txt") {
    fileType = "txt"; // text/plain has no magic bytes
  } else {
    throw new ValidationError(
      `Unsupported file type. Allowed: ${Object.values(ALLOWED_TYPES).flat().join(", ")}`,
    );
  }

  // 3. Store file via storage abstraction
  const filename = `${nanoid()}.${fileType}`;
  const storagePath = `${userId}/${filename}`;
  const storage = getStorage();
  await storage.save(storagePath, buffer);

  log.info({ requestId, userId, filename, fileType, fileSize: buffer.length }, "File stored");

  return { filename, fileType, storagePath, fileSize: buffer.length };
}

export async function deleteFile(storagePath: string, requestId: string) {
  const storage = getStorage();
  if (await storage.exists(storagePath)) {
    await storage.delete(storagePath);
    log.info({ requestId, storagePath }, "File deleted from storage");
  }
}
```

**Design Rationale:**
- `getStorage()` abstracts away whether we are on local disk or R2 — the service never imports `fs`.
- Magic byte validation prevents `.exe` renamed to `.pdf`.
- Per-user path prefix (`userId/filename`) provides clean isolation in both local and R2 storage.
- nanoid filename prevents filename collisions and path injection.
- `requestId` threaded through for structured logging.

### 6.3 Text Extraction — `src/services/extraction.service.ts`

```typescript
import pdfParse from "pdf-parse";
import mammoth from "mammoth";
import { getStorage } from "../lib/storage.js";
import { log } from "../middleware/logger.js";

type ExtractedPage = {
  pageNumber: number;
  text: string;
};

export async function extractText(
  storagePath: string,
  fileType: string,
  requestId: string,
): Promise<ExtractedPage[]> {
  const storage = getStorage();
  const buffer = await storage.read(storagePath);

  log.debug({ requestId, storagePath, fileType, bufferSize: buffer.length }, "Starting text extraction");

  let pages: ExtractedPage[];
  switch (fileType) {
    case "pdf":
      pages = await extractPdf(buffer);
      break;
    case "docx":
      pages = await extractDocx(buffer);
      break;
    case "txt":
      pages = [{ pageNumber: 1, text: buffer.toString("utf-8") }];
      break;
    default:
      throw new Error(`Unsupported file type: ${fileType}`);
  }

  log.info({ requestId, storagePath, pageCount: pages.length }, "Text extraction complete");
  return pages;
}

/**
 * LIMITATION: pdf-parse does not provide true page boundaries.
 * It concatenates all text and reports total page count. The page splitting
 * below is an approximation based on dividing total characters evenly.
 *
 * For production-grade page detection, consider:
 * - `pdf2json` — provides per-page text with layout coordinates
 * - `pdfjs-dist` (Mozilla's PDF.js) — getTextContent() per page object
 *
 * We keep pdf-parse for the showcase because it is pure JS with no native
 * dependencies, but citations referencing specific page numbers should be
 * treated as approximate.
 */
async function extractPdf(buffer: Buffer): Promise<ExtractedPage[]> {
  const data = await pdfParse(buffer);
  const text = data.text;
  const numpages = data.numpages;

  if (numpages <= 1) {
    return [{ pageNumber: 1, text }];
  }

  // Approximate page splitting by character count
  const charsPerPage = Math.ceil(text.length / numpages);
  const pages: ExtractedPage[] = [];
  for (let i = 0; i < numpages; i++) {
    const start = i * charsPerPage;
    const end = Math.min(start + charsPerPage, text.length);
    const pageText = text.slice(start, end).trim();
    if (pageText) {
      pages.push({ pageNumber: i + 1, text: pageText });
    }
  }
  return pages;
}

async function extractDocx(buffer: Buffer): Promise<ExtractedPage[]> {
  const result = await mammoth.extractRawText({ buffer });
  return [{ pageNumber: 1, text: result.value }];
}
```

**Design Rationale:**
- File read goes through `StorageAdapter` — works with local or R2 without code changes.
- The PDF page-boundary limitation is documented explicitly with alternative library suggestions for production use.
- Each extraction function returns the same `ExtractedPage[]` shape — the chunker does not need to know the source format.
- Structured logging at each stage with `requestId` for pipeline traceability.

### 6.4 Text Chunking — `src/services/chunking.service.ts`

```typescript
import { log } from "../middleware/logger.js";

type Chunk = {
  text: string;
  pageNumber?: number;
  index: number;
};

const DEFAULT_CHUNK_SIZE = 1000;  // characters
const DEFAULT_OVERLAP = 200;

export function chunkText(
  pages: { pageNumber: number; text: string }[],
  requestId: string,
  chunkSize = DEFAULT_CHUNK_SIZE,
  overlap = DEFAULT_OVERLAP,
): Chunk[] {
  const chunks: Chunk[] = [];
  let globalIndex = 0;

  for (const page of pages) {
    const paragraphs = page.text.split(/\n\n+/);
    let buffer = "";

    for (const para of paragraphs) {
      if (buffer.length + para.length > chunkSize && buffer.length > 0) {
        chunks.push({
          text: buffer.trim(),
          pageNumber: page.pageNumber,
          index: globalIndex++,
        });
        // Keep overlap from end of previous chunk
        buffer = buffer.slice(-overlap) + "\n\n" + para;
      } else {
        buffer += (buffer ? "\n\n" : "") + para;
      }
    }

    // Flush remaining buffer
    if (buffer.trim()) {
      chunks.push({
        text: buffer.trim(),
        pageNumber: page.pageNumber,
        index: globalIndex++,
      });
    }
  }

  log.info({ requestId, chunkCount: chunks.length, chunkSize, overlap }, "Chunking complete");
  return chunks;
}
```

**Design Rationale:**
- Paragraph-aware splitting — doesn't break mid-sentence.
- Overlapping chunks ensure context isn't lost at boundaries.
- Page number tracked per chunk — enables approximate citations.
- Simple implementation — no LangChain dependency for basic chunking.
- `requestId` logged for pipeline correlation.

### 6.5 Vector Store Service — `src/services/vector-store.service.ts`

```typescript
import { ChromaClient, Collection } from "chromadb";
import { getEmbeddingProvider } from "./llm/factory.js";
import { nanoid } from "nanoid";
import { log } from "../middleware/logger.js";

const COLLECTION_NAME = "document_chunks";

let client: ChromaClient | null = null;
let collection: Collection | null = null;

async function getCollection(): Promise<Collection> {
  if (collection) return collection;

  client = new ChromaClient({ path: "./data/chroma" });
  collection = await client.getOrCreateCollection({
    name: COLLECTION_NAME,
    metadata: { "hnsw:space": "cosine" },
  });

  return collection;
}

export async function addChunks(
  chunks: { text: string; pageNumber?: number; index: number }[],
  metadata: { userId: string; documentId: string; filename: string },
  requestId: string,
): Promise<string[]> {
  const col = await getCollection();
  const embedder = getEmbeddingProvider();

  // Embed all chunks with concurrency control (default 5 concurrent)
  // embedBatch handles batching internally — do NOT use Promise.all here
  const texts = chunks.map((c) => c.text);
  const embeddings = await embedder.embedBatch(texts);

  log.info(
    { requestId, documentId: metadata.documentId, chunkCount: chunks.length },
    "Chunk embeddings complete",
  );

  const ids = chunks.map(() => nanoid());

  await col.add({
    ids,
    embeddings,
    documents: chunks.map((c) => c.text),
    metadatas: chunks.map((c) => ({
      userId: metadata.userId,
      documentId: metadata.documentId,
      filename: metadata.filename,
      pageNumber: c.pageNumber ?? 0,
      chunkIndex: c.index,
    })),
  });

  log.info(
    { requestId, documentId: metadata.documentId, vectorCount: ids.length },
    "Vectors indexed in ChromaDB",
  );

  return ids;
}

export async function queryChunks(
  query: string,
  userId: string,
  topK = 5,
): Promise<{ text: string; filename: string; pageNumber: number; relevance: number }[]> {
  const col = await getCollection();
  const embedder = getEmbeddingProvider();

  const queryEmbedding = await embedder.embed(query);

  const results = await col.query({
    queryEmbeddings: [queryEmbedding],
    nResults: topK,
    where: { userId },  // Per-user isolation
  });

  if (!results.documents[0]) return [];

  return results.documents[0].map((doc, i) => ({
    text: doc ?? "",
    filename: (results.metadatas[0][i] as Record<string, unknown>).filename as string,
    pageNumber: (results.metadatas[0][i] as Record<string, unknown>).pageNumber as number,
    relevance: results.distances ? 1 - (results.distances[0][i] ?? 0) : 0,
  }));
}

export async function deleteDocumentChunks(documentId: string, requestId: string) {
  const col = await getCollection();
  await col.delete({ where: { documentId } });
  log.info({ requestId, documentId }, "Vectors deleted from ChromaDB");
}
```

**Design Rationale:**
- **`getEmbeddingProvider()`** — not `getChatProvider()`. Embeddings always use Gemini (`text-embedding-004`) regardless of whether the chat provider has fallen back to OpenRouter. Mixing embedding models would produce incompatible vector spaces.
- **`embedBatch()`** with built-in concurrency control (default 5 concurrent) — replaces `Promise.all` which would fire all requests simultaneously and hit Gemini's rate limits on large documents. The `EmbeddingProvider.embedBatch()` method (defined in S1's `base.ts`) processes chunks in batches of `concurrency` size.
- ChromaDB in embedded mode — runs in-process, persists to `./data/chroma`.
- `where: { userId }` filter on every query — ensures user A never sees user B's documents.
- Cosine similarity (`hnsw:space: "cosine"`) — standard for text embeddings.
- Returns `relevance` score (1 - distance) for frontend display.

### 6.6 Document Processing Pipeline — `src/services/document.service.ts`

```typescript
import { nanoid } from "nanoid";
import { eq } from "drizzle-orm";
import { db } from "../db/index.js";
import { documents, documentChunks } from "../db/schema.js";
import { validateAndStore, deleteFile } from "./upload.service.js";
import { extractText } from "./extraction.service.js";
import { chunkText } from "./chunking.service.js";
import * as vectorStore from "./vector-store.service.js";
import { NotFoundError, ForbiddenError } from "../lib/errors.js";
import { log } from "../middleware/logger.js";

export async function processUpload(
  buffer: Buffer,
  originalFilename: string,
  userId: string,
  requestId: string,
) {
  log.info({ requestId, userId, originalFilename, fileSize: buffer.length }, "Upload started");

  // 1. Validate and store file
  const { filename, fileType, storagePath, fileSize } = await validateAndStore(
    buffer,
    originalFilename,
    userId,
    requestId,
  );

  // 2. Create document record
  const docId = nanoid();

  db.transaction((tx) => {
    tx.insert(documents).values({
      id: docId,
      userId,
      filename,
      originalFilename,
      fileType,
      fileSize,
      storagePath,
      status: "processing",
    }).run();
  });

  // 3. Process (synchronous for showcase — fast enough for small documents)
  try {
    // Extract text
    const pages = await extractText(storagePath, fileType, requestId);
    log.info({ requestId, docId, pageCount: pages.length }, "Extraction complete");

    // Chunk text
    const chunks = chunkText(pages, requestId);
    log.info({ requestId, docId, chunkCount: chunks.length }, "Chunking complete");

    // Embed and store in vector DB
    const chromaIds = await vectorStore.addChunks(chunks, {
      userId,
      documentId: docId,
      filename: originalFilename,
    }, requestId);
    log.info({ requestId, docId, vectorCount: chromaIds.length }, "Indexing complete");

    // Store chunks in SQL and update status — all in one transaction
    db.transaction((tx) => {
      for (let i = 0; i < chunks.length; i++) {
        tx.insert(documentChunks).values({
          id: nanoid(),
          documentId: docId,
          chunkIndex: chunks[i].index,
          content: chunks[i].text,
          pageNumber: chunks[i].pageNumber,
          tokenCount: Math.ceil(chunks[i].text.length / 4), // approx
          chromaId: chromaIds[i],
        }).run();
      }

      tx.update(documents)
        .set({ status: "indexed", chunkCount: chunks.length })
        .where(eq(documents.id, docId))
        .run();
    });

    log.info({ requestId, docId, status: "indexed" }, "Document processing complete");
  } catch (err) {
    const errorMessage = err instanceof Error ? err.message : "Unknown error";
    db.update(documents)
      .set({ status: "failed", errorMessage })
      .where(eq(documents.id, docId))
      .run();

    log.error({ requestId, docId, error: errorMessage }, "Document processing failed");
    throw err;
  }

  return db.select().from(documents).where(eq(documents.id, docId)).get()!;
}

export function listDocuments(
  userId: string,
  cursor?: string,
  limit = 20,
) {
  let query = db
    .select()
    .from(documents)
    .where(eq(documents.userId, userId))
    .orderBy(documents.createdAt)
    .limit(limit + 1); // Fetch one extra to determine if there is a next page

  if (cursor) {
    const cursorDoc = db.select().from(documents).where(eq(documents.id, cursor)).get();
    if (cursorDoc) {
      query = db
        .select()
        .from(documents)
        .where(
          and(
            eq(documents.userId, userId),
            gt(documents.createdAt, cursorDoc.createdAt),
          ),
        )
        .orderBy(documents.createdAt)
        .limit(limit + 1);
    }
  }

  const rows = query.all();
  const hasMore = rows.length > limit;
  const data = hasMore ? rows.slice(0, limit) : rows;
  const nextCursor = hasMore ? data[data.length - 1].id : null;

  return { data, nextCursor };
}

export function getDocument(id: string, userId: string) {
  const doc = db.select().from(documents).where(eq(documents.id, id)).get();
  if (!doc) throw new NotFoundError("Document");
  if (doc.userId !== userId) throw new ForbiddenError();
  return doc;
}

export async function deleteDocument(id: string, userId: string, requestId: string) {
  const doc = getDocument(id, userId);

  // Delete from vector store
  await vectorStore.deleteDocumentChunks(id, requestId);

  // Delete file from storage
  await deleteFile(doc.storagePath, requestId);

  // Delete from SQL (cascades to chunks)
  db.delete(documents).where(eq(documents.id, id)).run();

  log.info({ requestId, docId: id }, "Document deleted");
}
```

Add the required import at the top:

```typescript
import { and, eq, gt } from "drizzle-orm";
```

**Design Rationale:**
- **`db.transaction()`** wraps the multi-table writes (insert chunks + update document status) — if any chunk insert fails, the entire batch rolls back and the document stays in `processing` state rather than ending up partially indexed.
- **Cursor-based pagination** on `listDocuments` — fetches `limit + 1` rows to determine `nextCursor`, consistent with the `paginationSchema` from `@chatbot/shared`. No offset-based pagination.
- **Structured logging at every stage** — upload started, extraction complete, chunking complete, indexing complete, failure. Each log entry includes `requestId` and `docId` for end-to-end tracing.
- **`deleteFile` is async** — the storage adapter may be R2, which requires network I/O.

### 6.7 RAG-Enhanced Chat — Updated Chat Route

```typescript
// Key additions to chat.routes.ts (building on S2 chat route)

import * as vectorStore from "../services/vector-store.service.js";

// Inside the message handler, before calling LLM:

const RAG_SYSTEM_PROMPT = `You are a helpful assistant with access to the user's documents.
When answering questions, cite your sources using [Source: filename, page X] format.
If the retrieved documents don't contain relevant information, say so honestly.`;

// Check if RAG is enabled for this message
const useRag = input.useDocuments ?? false;

if (useRag) {
  // Retrieve relevant chunks
  const relevantChunks = await vectorStore.queryChunks(input.content, userId, 5);

  if (relevantChunks.length > 0) {
    // Build context from retrieved chunks
    const context = relevantChunks
      .map((chunk, i) =>
        `[Document ${i + 1}: ${chunk.filename}, Page ${chunk.pageNumber}]\n${chunk.text}`,
      )
      .join("\n\n---\n\n");

    // Prepend RAG system prompt + context
    llmMessages.unshift({
      role: "system",
      content: `${RAG_SYSTEM_PROMPT}\n\n--- Retrieved Documents ---\n${context}`,
    });

    // Send citations via SSE before the response
    for (const chunk of relevantChunks) {
      await stream.writeSSE({
        event: "citation",
        data: JSON.stringify({
          source: chunk.filename,
          page: chunk.pageNumber,
          relevance: Math.round(chunk.relevance * 100) / 100,
        }),
      });
    }
  }
}
```

### 6.8 Document Routes — `src/routes/document.routes.ts`

```typescript
import { Hono } from "hono";
import { zValidator } from "@hono/zod-validator";
import { paginationSchema, documentSearchSchema } from "@chatbot/shared";
import { requireAuth } from "../middleware/auth.js";
import { rateLimit } from "../middleware/rate-limiter.js";
import { env } from "../env.js";
import * as docService from "../services/document.service.js";
import * as vectorStore from "../services/vector-store.service.js";
import { log } from "../middleware/logger.js";
import type { AppEnv } from "../app.js";

const docs = new Hono<AppEnv>();

// Rate limiter for uploads: default 20 per hour per user
const uploadLimiter = rateLimit("upload", {
  windowMs: 60 * 60 * 1000, // 1 hour
  max: env.RATE_LIMIT_UPLOAD_PER_HOUR,
  keyFn: (c) => c.get("userId"),
  message: "Upload limit reached. Please try again later.",
});

// Upload document
docs.post("/documents/upload", requireAuth, uploadLimiter, async (c) => {
  const userId = c.get("userId");
  const requestId = c.get("requestId");
  const formData = await c.req.formData();
  const file = formData.get("file");

  if (!file || !(file instanceof File)) {
    return c.json(
      { error: "No file provided", code: "VALIDATION_ERROR", requestId },
      422,
    );
  }

  const buffer = Buffer.from(await file.arrayBuffer());
  const doc = await docService.processUpload(buffer, file.name, userId, requestId);
  return c.json({ data: doc }, 201);
});

// List user's documents (cursor-based pagination)
docs.get(
  "/documents",
  requireAuth,
  zValidator("query", paginationSchema),
  (c) => {
    const userId = c.get("userId");
    const { cursor, limit } = c.req.valid("query");
    const result = docService.listDocuments(userId, cursor, limit);
    return c.json({ data: result.data, nextCursor: result.nextCursor });
  },
);

// Get document details
docs.get("/documents/:id", requireAuth, (c) => {
  const userId = c.get("userId");
  const requestId = c.get("requestId");
  const doc = docService.getDocument(c.req.param("id"), userId);
  return c.json({ data: doc });
});

// Get document processing status
docs.get("/documents/:id/status", requireAuth, (c) => {
  const userId = c.get("userId");
  const doc = docService.getDocument(c.req.param("id"), userId);
  return c.json({
    data: {
      status: doc.status,
      chunkCount: doc.chunkCount,
      errorMessage: doc.errorMessage,
    },
  });
});

// Delete document
docs.delete("/documents/:id", requireAuth, async (c) => {
  const userId = c.get("userId");
  const requestId = c.get("requestId");
  await docService.deleteDocument(c.req.param("id"), userId, requestId);
  return c.json({ data: { deleted: true } });
});

// Search documents (standalone, outside of chat context)
docs.post(
  "/documents/search",
  requireAuth,
  zValidator("json", documentSearchSchema),
  async (c) => {
    const userId = c.get("userId");
    const requestId = c.get("requestId");
    const { query, topK } = c.req.valid("json");

    log.info({ requestId, userId, queryLength: query.length, topK }, "Document search");

    const results = await vectorStore.queryChunks(query, userId, topK);
    return c.json({ data: results });
  },
);

export { docs };
```

**Design Rationale:**
- **Rate limiting on upload** — `uploadLimiter` enforces `RATE_LIMIT_UPLOAD_PER_HOUR` (default 20) per user. Prevents abuse and runaway API costs from embedding.
- **`zValidator("query", paginationSchema)`** on `GET /documents` — validates cursor/limit from query string using the shared `paginationSchema`.
- **`zValidator("json", documentSearchSchema)`** on `POST /documents/search` — validates `query` (required, max 2000 chars) and `topK` (1-20, default 5). No more raw `c.req.json()`.
- **`requestId` included in every error response** — consistent with S1 error format `{ error, code, requestId, detail? }`.
- All error responses flow through the global `errorHandler` middleware (S1) which attaches `requestId` automatically.

### 6.9 Updated SSE Event Types

```
event: citation      <-- NEW: sent before response tokens
data: {"source": "report.pdf", "page": 3, "relevance": 0.92}

event: token
data: {"content": "Based on the report...", "finishReason": null}

event: done
data: {"finishReason": "stop", "usage": {...}}
```

The `citation` event is already handled by the SSE client defined in S1 (`onCitation` callback).

---

## 7. Frontend Components

### 7.1 New Files

```
frontend/src/
+-- components/
|   +-- documents/
|   |   +-- upload-panel.tsx        # Drag & drop upload area
|   |   +-- document-list.tsx       # List of uploaded documents
|   |   +-- document-item.tsx       # Single document with status/delete
|   |   +-- upload-progress.tsx     # Upload + processing progress
|   +-- chat/
|   |   +-- citation-badge.tsx      # [Source: file.pdf, p.3] badge
|   |   +-- rag-toggle.tsx          # Toggle "search my documents"
|   |   +-- chat-message.tsx        # UPDATED: render citations
+-- hooks/
|   +-- use-documents.ts            # TanStack Query hooks for documents
+-- app/
    +-- (chat)/
        +-- documents/
            +-- page.tsx             # Document library page
```

> **Note:** No `stores/document-store.ts` — document server state is managed entirely by TanStack Query hooks. Zustand remains for UI-only state (sidebar open, theme).

### 7.2 Document Hooks — TanStack Query — `src/hooks/use-documents.ts`

```typescript
import { useQuery, useMutation, useQueryClient } from "@tanstack/react-query";
import { api, ApiError } from "@/lib/api-client";
import type { DocumentListItem, PaginatedResponse } from "@chatbot/shared";

const API_BASE = process.env.NEXT_PUBLIC_API_URL ?? "http://localhost:8000/api/v1";

// ── Query: document list ────────────────────────────
export function useDocuments(limit = 20) {
  return useQuery({
    queryKey: ["documents", { limit }],
    queryFn: () =>
      api.get<PaginatedResponse<DocumentListItem>>(`/documents?limit=${limit}`),
    staleTime: 30_000, // 30 seconds
  });
}

// ── Mutation: upload ────────────────────────────────
export function useUploadDocument() {
  const queryClient = useQueryClient();

  return useMutation({
    mutationFn: async (file: File) => {
      const formData = new FormData();
      formData.append("file", file);

      // Use raw fetch for FormData (api-client sets Content-Type: application/json)
      const res = await fetch(
        `${API_BASE}/documents/upload`,
        {
          method: "POST",
          credentials: "include", // httpOnly cookie auth
          body: formData,
          // Do NOT set Content-Type — browser sets multipart boundary automatically
        },
      );

      if (!res.ok) {
        const body = await res.json().catch(() => ({ error: "Upload failed", code: "UNKNOWN" }));
        throw new ApiError(res.status, body.code, body.error, body.requestId);
      }

      const json = await res.json();
      return json.data;
    },
    onSuccess: () => {
      // Invalidate document list — triggers refetch
      queryClient.invalidateQueries({ queryKey: ["documents"] });
    },
  });
}

// ── Mutation: delete ────────────────────────────────
export function useDeleteDocument() {
  const queryClient = useQueryClient();

  return useMutation({
    mutationFn: (id: string) => api.delete(`/documents/${id}`),
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ["documents"] });
    },
  });
}

// ── Query: document status (polling) ────────────────
export function useDocumentStatus(id: string, enabled: boolean) {
  return useQuery({
    queryKey: ["documents", id, "status"],
    queryFn: () =>
      api.get<{ status: string; chunkCount: number; errorMessage: string | null }>(
        `/documents/${id}/status`,
      ),
    enabled,
    refetchInterval: (query) => {
      // Poll every 2s while processing, stop when indexed/failed
      const status = query.state.data?.status;
      return status === "processing" ? 2000 : false;
    },
  });
}
```

**Design Rationale:**
- **TanStack Query replaces the old Zustand `useDocumentStore`** — document list is server state and belongs in the query cache, not in a client store. TanStack Query provides automatic caching, background refetching, stale-while-revalidate, and proper loading/error states.
- **`credentials: "include"`** on the upload fetch — httpOnly cookies sent automatically. **No** `localStorage.getItem("access_token")`, **no** `Authorization: Bearer` header. This follows the cookie-based auth pattern established in S1.
- **`queryClient.invalidateQueries`** on upload/delete success — document list re-fetches automatically, keeping the UI in sync without manual state management.
- **Polling for status** — `useDocumentStatus` polls every 2 seconds while `status === "processing"`, stops automatically when the document is indexed or failed.
- The raw `fetch` call for upload is necessary because the shared `api` client sets `Content-Type: application/json`, but FormData requires the browser to set the multipart boundary automatically.

### 7.3 Upload Panel — `src/components/documents/upload-panel.tsx`

```tsx
"use client";

import { useCallback, useState } from "react";
import { Upload } from "lucide-react";
import { useUploadDocument } from "@/hooks/use-documents";

const ACCEPTED_TYPES = {
  "application/pdf": [".pdf"],
  "application/vnd.openxmlformats-officedocument.wordprocessingml.document": [".docx"],
  "text/plain": [".txt"],
};

export function UploadPanel() {
  const uploadMutation = useUploadDocument();
  const [dragOver, setDragOver] = useState(false);

  const handleFile = useCallback(
    async (file: File) => {
      try {
        await uploadMutation.mutateAsync(file);
      } catch {
        // Error handled by TanStack Query — accessible via uploadMutation.error
      }
    },
    [uploadMutation],
  );

  const handleDrop = useCallback(
    async (e: React.DragEvent) => {
      e.preventDefault();
      setDragOver(false);
      const file = e.dataTransfer.files[0];
      if (file) await handleFile(file);
    },
    [handleFile],
  );

  const handleFileSelect = useCallback(
    async (e: React.ChangeEvent<HTMLInputElement>) => {
      const file = e.target.files?.[0];
      if (file) await handleFile(file);
    },
    [handleFile],
  );

  return (
    <div
      className={`relative rounded-lg border-2 border-dashed p-8 text-center transition-colors ${
        dragOver ? "border-primary bg-primary/5" : "hover:border-primary/50"
      }`}
      onDrop={handleDrop}
      onDragOver={(e) => { e.preventDefault(); setDragOver(true); }}
      onDragLeave={() => setDragOver(false)}
    >
      <Upload className="mx-auto h-10 w-10 text-muted-foreground" />
      <p className="mt-2 text-sm font-medium">
        {uploadMutation.isPending ? "Processing..." : "Drop files here or click to upload"}
      </p>
      <p className="mt-1 text-xs text-muted-foreground">PDF, DOCX, TXT -- max 25MB</p>

      {uploadMutation.isError && (
        <p className="mt-2 text-sm text-destructive">
          {uploadMutation.error instanceof Error
            ? uploadMutation.error.message
            : "Upload failed"}
        </p>
      )}

      <input
        type="file"
        accept=".pdf,.docx,.txt"
        onChange={handleFileSelect}
        className="absolute inset-0 cursor-pointer opacity-0"
        disabled={uploadMutation.isPending}
      />
    </div>
  );
}
```

**Design Rationale:**
- Uses `useUploadDocument()` mutation hook — no Zustand store. Upload state (`isPending`, `isError`, `error`) comes directly from TanStack Query.
- `credentials: "include"` is handled inside the hook — the component never touches auth.
- Drag-over visual feedback via local `useState` — this is ephemeral UI state, appropriate for component-local state.
- Error display uses `uploadMutation.error` — TanStack Query retains error state until the next mutation attempt.

### 7.4 Document List — `src/components/documents/document-list.tsx`

```tsx
"use client";

import { useDocuments } from "@/hooks/use-documents";
import { DocumentItem } from "./document-item";

export function DocumentList() {
  const { data, isLoading, isError, error } = useDocuments();

  if (isLoading) {
    return (
      <div className="space-y-2 p-4">
        {Array.from({ length: 3 }).map((_, i) => (
          <div key={i} className="h-16 animate-pulse rounded-lg bg-muted" />
        ))}
      </div>
    );
  }

  if (isError) {
    return (
      <p className="p-4 text-sm text-destructive">
        Failed to load documents: {error instanceof Error ? error.message : "Unknown error"}
      </p>
    );
  }

  const documents = data?.data ?? [];

  if (documents.length === 0) {
    return (
      <p className="p-4 text-sm text-muted-foreground">
        No documents uploaded yet. Upload a PDF, DOCX, or TXT file to get started.
      </p>
    );
  }

  return (
    <div className="space-y-2 p-4">
      {documents.map((doc) => (
        <DocumentItem key={doc.id} document={doc} />
      ))}
    </div>
  );
}
```

### 7.5 Document Item — `src/components/documents/document-item.tsx`

```tsx
"use client";

import { FileText, Trash2, Loader2, CheckCircle, XCircle } from "lucide-react";
import { Button } from "@/components/ui/button";
import { useDeleteDocument, useDocumentStatus } from "@/hooks/use-documents";
import type { DocumentListItem } from "@chatbot/shared";

function formatFileSize(bytes: number): string {
  if (bytes < 1024) return `${bytes} B`;
  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;
  return `${(bytes / (1024 * 1024)).toFixed(1)} MB`;
}

const STATUS_ICONS = {
  processing: <Loader2 className="h-4 w-4 animate-spin text-yellow-500" />,
  indexed: <CheckCircle className="h-4 w-4 text-green-500" />,
  failed: <XCircle className="h-4 w-4 text-destructive" />,
};

type Props = {
  document: DocumentListItem;
};

export function DocumentItem({ document: doc }: Props) {
  const deleteMutation = useDeleteDocument();

  // Poll status while processing
  const { data: statusData } = useDocumentStatus(
    doc.id,
    doc.status === "processing",
  );

  const currentStatus = statusData?.status ?? doc.status;

  return (
    <div className="flex items-center gap-3 rounded-lg border p-3">
      <FileText className="h-8 w-8 shrink-0 text-muted-foreground" />
      <div className="min-w-0 flex-1">
        <p className="truncate text-sm font-medium">{doc.originalFilename}</p>
        <div className="flex items-center gap-2 text-xs text-muted-foreground">
          <span>{doc.fileType.toUpperCase()}</span>
          <span>{formatFileSize(doc.fileSize)}</span>
          {currentStatus === "indexed" && (
            <span>{statusData?.chunkCount ?? doc.chunkCount} chunks</span>
          )}
        </div>
      </div>
      <div className="flex items-center gap-2">
        {STATUS_ICONS[currentStatus as keyof typeof STATUS_ICONS]}
        <Button
          variant="ghost"
          size="icon"
          className="h-8 w-8 text-muted-foreground hover:text-destructive"
          onClick={() => deleteMutation.mutate(doc.id)}
          disabled={deleteMutation.isPending}
        >
          <Trash2 className="h-4 w-4" />
        </Button>
      </div>
    </div>
  );
}
```

### 7.6 Citation Badge — `src/components/chat/citation-badge.tsx`

```tsx
import { FileText } from "lucide-react";

type Props = {
  source: string;
  page: number;
  relevance: number;
};

export function CitationBadge({ source, page, relevance }: Props) {
  return (
    <span className="inline-flex items-center gap-1 rounded-md bg-primary/10 px-2 py-0.5 text-xs text-primary">
      <FileText className="h-3 w-3" />
      {source}
      {page > 0 && <span className="text-muted-foreground">p.{page}</span>}
      <span className="text-muted-foreground">{Math.round(relevance * 100)}%</span>
    </span>
  );
}
```

### 7.7 RAG Toggle in Chat Input

```tsx
// Added to chat-input.tsx
<div className="flex items-center gap-2 px-3 py-1 border-t">
  <label className="flex items-center gap-1.5 text-xs text-muted-foreground cursor-pointer">
    <input
      type="checkbox"
      checked={useDocuments}
      onChange={(e) => setUseDocuments(e.target.checked)}
      className="rounded"
    />
    Search my documents
  </label>
</div>
```

### 7.8 Updated SSE Client — Citation Events

The SSE client from S1 already handles `citation` events via the `onCitation` callback. No changes needed:

```typescript
// Already defined in S1 sse-client.ts
type SSECallbacks = {
  onToken: (content: string) => void;
  onDone: (usage: Record<string, unknown>) => void;
  onError: (error: string, code?: string) => void;
  onTitle?: (title: string) => void;
  onCitation?: (citation: { source: string; page: number; relevance: number }) => void;
  onInfo?: (message: string) => void;
};

// In parser switch:
case "citation":
  callbacks.onCitation?.(data);
  break;
```

### 7.9 Document Library Page — `src/app/(chat)/documents/page.tsx`

```tsx
"use client";

import { UploadPanel } from "@/components/documents/upload-panel";
import { DocumentList } from "@/components/documents/document-list";

export default function DocumentsPage() {
  return (
    <div className="mx-auto max-w-3xl space-y-6 p-6">
      <div>
        <h1 className="text-2xl font-bold">Document Library</h1>
        <p className="mt-1 text-sm text-muted-foreground">
          Upload documents to enable AI-powered search and Q&A across your files.
        </p>
      </div>
      <UploadPanel />
      <DocumentList />
    </div>
  );
}
```

---

## 8. Data Flow: Question About a Document

```
1. User enables "Search my documents" toggle
2. User types: "What are the key findings in the Q4 report?"
3. Frontend sends: POST /conversations/:id/messages
   { content: "What are the key findings...", useDocuments: true }
   (with credentials: "include" — httpOnly cookie auth)
4. Backend:
   a. Embeds the question via getEmbeddingProvider() (Gemini text-embedding-004)
   b. Queries ChromaDB for top-5 similar chunks (filtered by userId)
   c. Sends citation SSE events for each relevant chunk
   d. Builds LLM prompt: system prompt + retrieved context + conversation history
   e. Streams Gemini response via SSE tokens (using getChatProvider())
   f. Persists user message + assistant response to DB
5. Frontend:
   a. Receives citation events --> renders CitationBadge components below message
   b. Receives token events --> renders streamed response
   c. Response naturally includes [Source: file, page] references
```

---

## 9. Environment Variables (S3 additions)

S1 already defines these in `src/env.ts`. Listed here for reference:

| Variable | Default | Purpose |
|----------|---------|---------|
| `STORAGE_TYPE` | `"local"` | `"local"` or `"r2"` — file storage backend |
| `UPLOAD_DIR` | `"./data/uploads"` | Local storage directory (when `STORAGE_TYPE=local`) |
| `UPLOAD_MAX_SIZE_MB` | `25` | Maximum upload file size |
| `RATE_LIMIT_UPLOAD_PER_HOUR` | `20` | Uploads per user per hour |
| `EMBEDDING_PROVIDER` | `"gemini"` | Always Gemini for embeddings |
| `GEMINI_EMBEDDING_MODEL` | `"text-embedding-004"` | Embedding model |
| `R2_ACCOUNT_ID` | — | Cloudflare R2 (when `STORAGE_TYPE=r2`) |
| `R2_ACCESS_KEY` | — | Cloudflare R2 |
| `R2_SECRET_KEY` | — | Cloudflare R2 |
| `R2_BUCKET` | — | Cloudflare R2 |

---

## 10. Testing Strategy (S3)

| Area | Tests |
|------|-------|
| File upload | Valid PDF/DOCX/TXT accepted, .exe rejected, oversized rejected |
| Magic byte validation | Renamed .exe to .pdf detected and rejected |
| Storage adapter | `save()`, `read()`, `delete()`, `exists()` for local adapter |
| Text extraction | PDF with 3 pages produces 3 ExtractedPage objects |
| Chunking | 3000-char text produces ~3 chunks with correct overlap |
| Embedding provider | `getEmbeddingProvider()` returns `GeminiEmbeddingProvider`, NOT chat provider |
| embedBatch concurrency | 20 chunks with concurrency=5 produces 4 batches, not 20 parallel calls |
| Vector store | Add chunks, query returns relevant results |
| User isolation | User A's query never returns User B's chunks |
| Document CRUD | Create, list (cursor pagination), get, delete — ownership enforced |
| Database transactions | Failure mid-chunk-insert rolls back all chunk inserts |
| Rate limiting | 21st upload in an hour returns 429 |
| RAG integration | Question about uploaded doc returns answer with citations |
| Error format | All error responses include `{ error, code, requestId }` |
| Search validation | Invalid search body returns 422 with field errors |
| Cookie auth | Upload with `credentials: "include"` succeeds; `Authorization: Bearer` not used |

---

## 11. Security Considerations

- **Path traversal prevention**: nanoid filenames, no user input in storage paths.
- **File type validation**: Magic bytes via `file-type` library, not just extension.
- **Size limits**: Validated before processing, configurable via `UPLOAD_MAX_SIZE_MB`.
- **User isolation**: ChromaDB `where: { userId }` on every query — user A never sees user B's data.
- **No arbitrary code execution**: PDF/DOCX parsing uses safe JS libraries (no shell commands).
- **Rate limiting on upload**: `RATE_LIMIT_UPLOAD_PER_HOUR` prevents abuse and runaway embedding API costs.
- **Cookie-based auth**: No tokens in `localStorage` or `Authorization` headers — XSS cannot steal auth credentials.
- **Storage abstraction**: File operations go through `StorageAdapter` — no direct `fs` calls in service code. R2 adapter uses presigned URLs for secure access.
- **Transaction safety**: Multi-table writes wrapped in `db.transaction()` — prevents partially-indexed documents.
- **Input validation**: All endpoints use `zValidator` with shared Zod schemas — no raw JSON parsing.

---

## 12. Definition of Done

Phase S3 is complete when:

1. Drag & drop upload works for PDF, DOCX, TXT files via `StorageAdapter`
2. Invalid file types rejected with clear error message (magic byte validation)
3. Upload rate limiting enforced (20/hour per user, configurable)
4. Document library page lists all uploaded documents with status via TanStack Query
5. Cursor-based pagination on `GET /documents` using `paginationSchema`
6. Deleting a document removes it from storage, SQLite, and ChromaDB
7. Document processing pipeline logs every stage with `requestId` (pino)
8. Multi-table writes (chunks + status update) wrapped in `db.transaction()`
9. Asking a question with "Search my documents" enabled returns relevant context
10. Citations displayed as badges below the assistant's message
11. Citations include filename, page number, and relevance score
12. User A cannot search User B's documents
13. Processing a 10-page PDF completes within 30 seconds
14. `POST /documents/search` validates body with `documentSearchSchema` (Zod)
15. Embedding uses `getEmbeddingProvider()` (never `getChatProvider()`)
16. `embedBatch()` concurrency control used (not `Promise.all`)
17. All error responses include `{ error, code, requestId }` format
18. Frontend uses `credentials: "include"` — no `localStorage` token access
19. No Zustand store for document list — server state managed by TanStack Query
